------

[TOC]

---

# Context 

**company report summary‚Äì GenAI use-case:**

company has an publication (attached) that they manually type up and publish 1x every monthly/quaterly/yearly. Multiple people submit sections of the attached document and someone manually pieces it all together to review before its published to the public. The input data comes from an application that hosts all of their goals and objectives for the year

**Key Points:**
Our current proposed solution is to train an LLM on historic input data, to be able to use GenAI to create a first draft of the language of the document attached. 
No charts, pictures, or graphics of any kind are in scope currently. 
Note the size of this output document is huge.  Can vary 100-150 pages depending on year. 
Must be local model as this trained data cannot risk hitting the internet before it is published. 
We need the model to learn over time to improve its outputs based on user-input. 
The data inputs are confidential. 



# Solution



## 1. Requirement Understading

#### Requirement 

Auto generate company's report

For each part,  using structured input data to generate standard paragraphs for report 



#### Key capaibilities Identified

- Extracting and Structuring Input data
- Text generation with fixed formatting

- Custim tone & content in science topics 

---

## 2. Approach - Build LLM App

### 1) Modelling 

#### a. prompt engineering (e.i. In-Context Learning)

‚Äã	Easiest approach, short term solution, decent performance score

-  Tuning-free Alignment Methods.

  - 1 Base Instruction

  - 3 Example Query & Answer

    ![image-20231206122737245](/Users/chuchwu/Library/Application Support/typora-user-images/image-20231206122737245.png)

‚Äã		Ref: [ https://doi.org/10.48550/arXiv.2312.01552](https://doi.org/10.48550/arXiv.2312.01552)

‚Äã		THE UNLOCKING SPELL ON BASE LLMS: RETHINKING ALIGNMENT VIA IN-CONTEXT LEARNING by Bill Yuchen Lin, Abhilasha Ravichander,  Allen Institute for Artificial Intelligence, 2023 Dec 

---

#### b. embedding modelling

‚Äã		‚Ä¢  feeding doc as embeddings

‚Äã		‚Ä¢ input API -> or self orgnizing to structural data JSON

‚Äã		‚Ä¢  generate consistent and accurate outputs

#### c. Fine-tuning

‚Äã	Time consuming, in need of large amount of GPU

---

### 2) Architechture

#### Local structure

![image-20231206121138016](/Users/chuchwu/Library/Application Support/typora-user-images/image-20231206121138016.png)

Ref: [The architecture of today's LLM applications - The GitHub Blog](https://github.blog/2023-10-30-the-architecture-of-todays-llm-applications/#:~:text=Five steps to building an LLM app 1,5. Conduct online evaluations of your app. )

---

#### Cloud Infrastructure

![img](https://miro.medium.com/v2/resize:fit:1400/0*d75K5WTkwyslFc6W.png)

---

## 3. Model Choice

#### Model Choice 

| Consideration            | Details                                                      |      |
| ------------------------ | :----------------------------------------------------------- | ---- |
| **Commercial Licensing** | [list of open LLMs that are licensed for commercial use](https://github.com/eugeneyan/open-llms). |      |
| **Model size**           | 7 to 175 billion                                             |      |
| **Model performance**    | pre-prudction tests on Model performance                     |      |

**pre-prudction tests on Model performance**

 - Coheriense 
 - Comprehensiveness
 - Speed/Latency 
 - GPU usage 

---

#### **Improvement Strategy** 

- ##### Improve on content generation

  Feed more context documentations.

  Build Embeddings based on the context.

  ![llm chatbot embedding database](https://i0.wp.com/bdtechtalks.com/wp-content/uploads/2023/05/llm-chatbot-embedding-database.jpg?resize=696%2C435&ssl=1)

  ---

- ##### Improve response speed 

  Skeleton of Thought decoding

![SoT - Figure 3: A bar plot showing the average speed-up of Skeleton-of-Thought on different models. Each bar corresponds to one model. The figure shows that Skeleton-of-Thought provides speed-up for all models. The speed-up ranges from 1.13x to 2.39x.](https://www.microsoft.com/en-us/research/uploads/prod/2023/11/SoT-blog_Figure3.png)

‚Äã	Accelerate the end-to-end generation of LLMs by 2x without any change to the model, system, or hardware



![A figure showing the difference between the normal sequential decoding approach and the Skeleton-of-Thought approach. Given a question, the left part of the figure shows that the normal sequential decoding approach generates the answer sequentially from the beginning to the end. The right part of the figure shows that the Skeleton-of-Thought approach first prompts the LLM to give a skeleton of answer and then expand multiple points in the skeleton in parallel to get the final answer.](https://www.microsoft.com/en-us/research/uploads/prod/2023/11/SoT-blog_Figure2.png)

Ref: [skeleton-of-thought | ü¶úÔ∏èüîó Langchain](https://python.langchain.com/docs/templates/skeleton-of-thought)



---

## 3.  Potential Issues & Mitigation

- #### Issues on Model side 

‚Äã	high GPU usage on High-dimensionality 

‚Äã	Out-of-vocabulary words

‚Äã	harmful/offensive content

‚Äã	wrong in number - 2M vs 20M

‚Äã	hallucination on Domain adaptation

---

- #### Mitigations

![image-20231206121043126](/Users/chuchwu/Library/Application Support/typora-user-images/image-20231206121043126.png)

Ref: https://eugeneyan.com/writing/llm-patterns/

---

- #### Security Implications & mitigations

| Issue                                                        | Mitigation                                                   |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| data exposed via LLM providers like OpenAI, Microsoft Azure, Google Cloud Platform, etc | Understand vendor‚Äôs license agreement                        |
| exposed via LLM-based apps to unauthorized users             | expected availability requirements (e.g., SLA) for all internal and external users |
| insecure source code                                         | vendor qualification                                         |

---

## 4. Timeline

| Week     | Action                                                       |
| -------- | ------------------------------------------------------------ |
| week 1   | document gathering feed initial data, test models, model choice |
| week 2,3 | in-context learning, embedding modelling                     |
| week 4   | evaluate and mitigate risks                                  |

---

## 5. Demo

```python
! pip install langchain
```



```python
from langchain.llms import Ollama

llm = Ollama(model="llama2")
llm('''
Ôªøexample input:  Goal: Mars 2020 instrument payload for
spacecraft integration- Do not exceed $500K of spend ‚Äì Achieved Y‚Äì Commentary on why it was or was not achieved.

exmaple output:
Goal 1.1.15: reduce carbon emission
FY 2019 Annual Indicator\n
Green
FY 2020‚Äì2021 Plan\n
No  goal after FY 2019. 
New performance goals for Strategic Objective 1.1 are on page 49.
FY 2019  Progress\n
achieved both the FY 2019 milestone and the FY 2018‚Äì2019 agency priority goal for the mission.

test input: goal:  two missions in support of bio science, Do not exceed $500K ‚Äì Achieved Y‚Äì Commentary on why it was or was not achieved.

test output:
please generate it.
Output format:
 Goal 1.1.15:\n
detailed content \n
FY 2019 Annual  Indicator\n
detailed content \n
FY 2020‚Äì2021  Plan\n
detailed content \n
FY 2019  Progress\n
detailed content\n
    ''')
```













